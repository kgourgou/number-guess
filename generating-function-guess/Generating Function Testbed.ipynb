{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from genfunc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.9986154276648289"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = np.array([\n",
    "            [1,0,0],\n",
    "            [0,1,0],\n",
    "            [0,0,1],\n",
    "            ])\n",
    "\n",
    "a = level(3,1)\n",
    "loss = sqeuclidean_loss(np.array([1,2,1]))\n",
    "a.child = loss\n",
    "loss.parent = a\n",
    "a.forward(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 5.998615427664829\n",
      "iteration 100 / 1000: loss 4.019381947499418\n",
      "iteration 200 / 1000: loss 2.6931933601506564\n",
      "iteration 300 / 1000: loss 1.8045785570769366\n",
      "iteration 400 / 1000: loss 1.2091607742860746\n",
      "iteration 500 / 1000: loss 0.8102001280788617\n",
      "iteration 600 / 1000: loss 0.5428759032698313\n",
      "iteration 700 / 1000: loss 0.3637548750453293\n",
      "iteration 800 / 1000: loss 0.24373454102912273\n",
      "iteration 900 / 1000: loss 0.16331472254021165\n"
     ]
    }
   ],
   "source": [
    "net = network(a,loss)\n",
    "net.input_val(np.array(F))\n",
    "loss_history = net.grad_desc(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(a.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, it got it. That was quite easy, of course. Let's do a harder one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.04329941051499"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = np.array([\n",
    "            [1,0,0,0,0,0], # 1\n",
    "            [0,1,0,0,0,0], # x\n",
    "            [0,0,1,0,0,0], # x^2\n",
    "            [1,1,1,1,1,1], # 1/(1-x)\n",
    "            [1,0,-1,0,1,0], # 1/(1+x^2)\n",
    "            ])\n",
    "\n",
    "a = level(5,1)\n",
    "loss = sqeuclidean_loss(np.array([3,6,-6,2,3,2]))\n",
    "a.child = loss\n",
    "loss.parent = a\n",
    "a.forward(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 98.04329941051499\n",
      "iteration 100 / 1000: loss 37.459464018226775\n",
      "iteration 200 / 1000: loss 21.644723869054875\n",
      "iteration 300 / 1000: loss 14.405077585310273\n",
      "iteration 400 / 1000: loss 10.165478135899571\n",
      "iteration 500 / 1000: loss 7.429194056080526\n",
      "iteration 600 / 1000: loss 5.589241841846901\n",
      "iteration 700 / 1000: loss 4.32551868940288\n",
      "iteration 800 / 1000: loss 3.44386886784497\n",
      "iteration 900 / 1000: loss 2.8189871131501305\n"
     ]
    }
   ],
   "source": [
    "net = network(a,loss)\n",
    "net.input_val(F)\n",
    "loss_history = net.grad_desc(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  5., -5.,  1.,  3.,  1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(a.W).dot(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 2.367964853218849\n",
      "iteration 100 / 1000: loss 2.035401673614263\n",
      "iteration 200 / 1000: loss 1.7841168280172424\n",
      "iteration 300 / 1000: loss 1.5890922178040419\n",
      "iteration 400 / 1000: loss 1.4334540462471004\n",
      "iteration 500 / 1000: loss 1.3057897243110943\n",
      "iteration 600 / 1000: loss 1.1983520023185437\n",
      "iteration 700 / 1000: loss 1.1058557955868369\n",
      "iteration 800 / 1000: loss 1.0246717302251693\n",
      "iteration 900 / 1000: loss 0.9522854303609513\n"
     ]
    }
   ],
   "source": [
    "net = network(a,loss)\n",
    "net.input_val(F)\n",
    "loss_history = net.grad_desc(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  6., -5.,  2.,  4.,  2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(a.W).dot(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 0.8869348642102926\n",
      "iteration 100 / 1000: loss 0.8273670153696908\n",
      "iteration 200 / 1000: loss 0.7726745280814349\n",
      "iteration 300 / 1000: loss 0.7221859588662615\n",
      "iteration 400 / 1000: loss 0.6753919658355226\n",
      "iteration 500 / 1000: loss 0.6318955955948791\n",
      "iteration 600 / 1000: loss 0.5913787331458484\n",
      "iteration 700 / 1000: loss 0.5535793971394074\n",
      "iteration 800 / 1000: loss 0.5182763163947284\n",
      "iteration 900 / 1000: loss 0.48527839863680755\n"
     ]
    }
   ],
   "source": [
    "net = network(a,loss)\n",
    "net.input_val(F)\n",
    "loss_history = net.grad_desc(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  6., -6.,  2.,  4.,  2.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(a.W).dot(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That one took many more iterations, but the number of parameters is so small that even doing tens of thousands of iterations won't be a problem. Using the sqeuclidean loss function, as soon as we get below 1/2 we know we've found something we can round accurately. Now let's try one it can't do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.028828107072201"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = np.array([\n",
    "            [1,0,0,0,0,0], # 1\n",
    "            [0,1,0,0,0,0], # x\n",
    "            [0,0,1,0,0,0], # x^2\n",
    "            [1,1,1,1,1,1], # 1/(1-x)\n",
    "            [1,0,-1,0,1,0], # 1/(1+x^2)\n",
    "            ])\n",
    "\n",
    "a = level(5,1)\n",
    "loss = sqeuclidean_loss(np.array([1,2,3,4,5,6]))\n",
    "a.child = loss\n",
    "loss.parent = a\n",
    "a.forward(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = network(a,loss)\n",
    "net.input_val(F)\n",
    "loss_history = net.grad_desc(num_iters=10000,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  3.,  5.,  5.,  5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(a.W).dot(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll never do better than this. In this case we'd be able to conclude that the closed form of the generating function is not a linear combination of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can setup multiple layers (although right now the only layer that works is multiplication by linear paramters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.99997355976808"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = np.array([\n",
    "            [1,0,0,0,0,0], # 1\n",
    "            [0,1,0,0,0,0], # x\n",
    "            [0,0,1,0,0,0], # x^2\n",
    "            [1,1,1,1,1,1], # 1/(1-x)\n",
    "            [1,0,-1,0,1,0], # 1/(1+x^2)\n",
    "            ])\n",
    "\n",
    "a = level(5,8)\n",
    "b = level(8,1)\n",
    "loss = sqeuclidean_loss(np.array([3,6,-6,2,3,2]))\n",
    "a.child = b\n",
    "b.parent = a\n",
    "b.child = loss\n",
    "loss.parent = b\n",
    "a.forward(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1000: loss 97.99997355976808\n",
      "iteration 100 / 1000: loss 97.84400360614269\n",
      "iteration 200 / 1000: loss 40.507714960351926\n",
      "iteration 300 / 1000: loss 4.227658415849827\n",
      "iteration 400 / 1000: loss 1.3012428348942475\n",
      "iteration 500 / 1000: loss 0.7618430509762748\n",
      "iteration 600 / 1000: loss 0.4507238610771433\n",
      "iteration 700 / 1000: loss 0.2613710978391413\n",
      "iteration 800 / 1000: loss 0.14880173873992297\n",
      "iteration 900 / 1000: loss 0.08340672780973207\n"
     ]
    }
   ],
   "source": [
    "net = network(a,loss)\n",
    "net.input_val(F)\n",
    "loss_history = net.grad_desc(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  6., -6.,  2.,  3.,  2.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(b.W.dot(a.W.dot(F)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is all just basic stuff so far - linear combinations of the inputs is just the beginning. We want a flexible way to test for compositions of our inputs as well. That's coming - the framework behind this is modularized, so adding additional levels is pretty easy, you just need to tell it how to auto-diff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One bit of weirdness - it converged much faster when we added more paramters, even though they were just as expressive as before. I think this is equivalent to messing with the learning rate, and might explain why some deeper networks seem to perform better, even though they're just as expressive (although they do have nonlinearities between parameter layers)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "anaconda python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
